{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78870970",
   "metadata": {},
   "source": [
    "## Assignment Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e74cb",
   "metadata": {},
   "source": [
    "#### 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2bb60",
   "metadata": {},
   "source": [
    "**Ans:** It is common to initailize neural network weights randomly, intializing all weights to the same value is not recommended , even if that value is randomly selected using He initialization.\n",
    "\n",
    "Initializing all weights to the same value would result in identical activations in the network's neurons. This means that during the forward propagation phase, all neurons would produce the same output, making the network redundant and limiting its learning capacity. The purpose of random initialization is to break the symmetry between neurons and allow them to learn distinct features and representations.\n",
    "\n",
    "It is essential to initialize the weights with different random values, even when using He initialization or any other weight initialization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2db7d",
   "metadata": {},
   "source": [
    "#### 2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907b0a3",
   "metadata": {},
   "source": [
    "**Ans:** Yes, it is generally acceptable to initialize the bias terms to 0. Initializing biases to 0 does not introduce symmetry issues or affect the learning capacity of the network.\n",
    "\n",
    "Biases are scalar values added to each neuron in a layer, and they serve to shift the activation function of the neurons. Initializing biases to 0 means that initially, the neurons are not biased towards any particular activation value. During the training process, the network will learn the appropriate biases for each neuron based on the data it processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b529c6c",
   "metadata": {},
   "source": [
    "#### 3.Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbb4e2",
   "metadata": {},
   "source": [
    "**Ans:** The SELU (Scaled Exponential Linear Unit) activation function offers several advantages over the commonly used ReLU (Rectified Linear Unit) activation function. Here are three advantages of SELU over ReLU:\n",
    "\n",
    "**Self-normalizing property:** One significant advantage of SELU is its self-normalizing property. In a deep neural network, the activations can either explode or vanish as the information propagates through the layers. SELU is designed to prevent this issue by ensuring that the mean and variance of the activations remain stable during training. This property allows for more efficient training of deep neural networks without the need for additional normalization techniques, such as batch normalization.\n",
    "\n",
    "**Non-zero output for negative values:** Unlike ReLU, which sets all negative inputs to zero, SELU provides a non-zero output for negative values. This can be beneficial in certain scenarios where it is important to preserve information from negative inputs. By allowing negative values to pass through, SELU can capture more nuanced patterns and gradients, potentially leading to improved performance in certain types of tasks.\n",
    "\n",
    "**Continuous and smooth gradient:** The SELU activation function is smooth and continuously differentiable across its entire range of inputs. In contrast, ReLU has a discontinuous gradient at the origin (where inputs are negative), which can cause issues during backpropagation. The smooth gradient of SELU can result in more stable and consistent updates to the weights during training, allowing for more reliable convergence and improved learning dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f49f4",
   "metadata": {},
   "source": [
    "#### 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6f3f8",
   "metadata": {},
   "source": [
    "**Ans:** \\\n",
    "**SELU (Scaled Exponential Linear Unit):** \\\n",
    "Deep neural networks: SELU is particularly useful in deep neural networks due to its self-normalizing property. It helps stabilize the mean and variance of activations, enabling more efficient training without the need for additional normalization techniques. \\\n",
    "Tasks with negative values: SELU allows negative values to pass through while maintaining non-zero outputs, which can be advantageous when negative information needs to be preserved.\n",
    "\n",
    "**Leaky ReLU and its variants:** \\\n",
    "Avoiding dead neurons: Leaky ReLU helps mitigate the \"dying ReLU\" problem, where ReLU neurons can become permanently inactive. By allowing a small negative slope for negative inputs, leaky ReLU ensures that all neurons can contribute to the learning process, preventing dead neurons. \\\n",
    "Improved gradient propagation: Leaky ReLU and its variants can facilitate better gradient flow during backpropagation compared to traditional ReLU, especially when dealing with deep networks.\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** \\\n",
    "General-purpose activation:* ReLU is widely used as a default choice for activation functions in deep learning. It is computationally efficient and introduces non-linearity into the network, which is crucial for learning complex representations. \\\n",
    "Sparse activations: ReLU tends to produce sparse activations, which can be beneficial for scenarios where sparsity is desirable or when dealing with high-dimensional inputs.\n",
    "\n",
    "**tanh (Hyperbolic tangent):** \\\n",
    "Symmetric activation: tanh produces values between -1 and 1, centered around 0. It is often used in scenarios where symmetric activation is desired, such as in recurrent neural networks (RNNs) or autoencoders. \\\n",
    "Capturing negative values: tanh can capture both positive and negative values, making it suitable for tasks where the input range spans both sides of zero.\n",
    "\n",
    "**logistic (Sigmoid):** \\\n",
    "Binary classification: The logistic function is commonly used in binary classification tasks where the output is required to be between 0 and 1, representing probabilities or binary decisions. \\\n",
    "Output probability mapping: logistic is suitable for mapping arbitrary real values to a probability range (0 to 1), making it useful in tasks like logistic regression or as the final layer activation in multi-label classification.\n",
    "\n",
    "**softmax:** \\\n",
    "Multi-class classification: Softmax is frequently used in multi-class classification problems, where the goal is to assign input samples to multiple exclusive classes. It produces a probability distribution over the classes, ensuring that the sum of the output probabilities is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839d12b",
   "metadata": {},
   "source": [
    "#### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe46425",
   "metadata": {},
   "source": [
    "**Ans:** When setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in Stochastic Gradient Descent (SGD) optimization, the following issues may arise:\n",
    "\n",
    "Overshooting: Momentum helps accelerate SGD by accumulating a fraction of the previous updates to determine the current update direction. When the momentum value is extremely close to 1, it means that the updates from previous iterations have a significant impact on the current update. This can lead to overshooting, where the optimizer overshoots the optimal solution and keeps oscillating around it, struggling to converge.\n",
    "\n",
    "Slow convergence or divergence: With a momentum value that is too close to 1, the updates become increasingly influenced by past gradients. This can cause the optimizer to continue moving in a particular direction even when it encounters a steep gradient that suggests changing course. As a result, the convergence can become slow, as the optimizer fails to respond effectively to local gradient information. In extreme cases, the optimizer may even diverge, leading to an unstable training process.\n",
    "\n",
    "Difficulty in escaping local minima: Higher momentum values allow the optimizer to escape shallow local minima and find a better solution. However, when the momentum is set very close to 1, it becomes less capable of escaping local minima. This happens because the momentum essentially becomes more persistent, preventing the optimizer from exploring alternative paths or making significant changes in the update direction.\n",
    "\n",
    "In general, momentum values close to 1 are not recommended unless there are specific requirements or scenarios where it has been observed to be beneficial. More commonly used values for momentum typically range from 0.9 to 0.99, striking a balance between utilizing past gradients and responsiveness to local gradients for efficient convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049760c6",
   "metadata": {},
   "source": [
    "#### 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4745d63",
   "metadata": {},
   "source": [
    "**Ans:** To produce a sparse model, where only a subset of the model's parameters are non-zero or active, several techniques can be employed. Here are three common ways to achieve sparsity in models:\n",
    "\n",
    "**L1 Regularization (Lasso regularization):** L1 regularization is a technique that adds a penalty term to the loss function of a model, encouraging the model to learn sparse representations. By adding the absolute values of the model's parameters as the regularization term, L1 regularization promotes the shrinking or elimination of irrelevant or less important features. This encourages the model to focus on a subset of the most relevant features, resulting in a sparse model.\n",
    "\n",
    "**Group Lasso Regularization:** Group Lasso regularization extends the concept of L1 regularization by promoting sparsity at the group level. Instead of penalizing individual parameters, it penalizes entire groups of parameters together. This is particularly useful when dealing with structured data where features or parameters can be grouped together, such as in image processing or natural language processing tasks. Group Lasso encourages the model to select entire groups of features while setting some groups to zero, resulting in sparsity at the group level.\n",
    "\n",
    "**Dropout:** Dropout is a technique primarily used during training in neural networks to introduce sparsity in activations and weights. During training, randomly selected units (neurons or inputs) are \"dropped out\" or temporarily set to zero with a certain probability. This forces the network to learn robust representations that do not rely on specific units and encourages the network to rely on a diverse set of features. Dropout can effectively regularize the model, prevent overfitting, and create a sparse model by producing a network that uses only a subset of units during each forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c428b2",
   "metadata": {},
   "source": [
    "#### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e208f",
   "metadata": {},
   "source": [
    "**Ans:** Dropout can slow down the training process because it reduces the effective capacity of the network, but it helps prevent overfitting. During inference, dropout does not slow down prediction as the entire network is used. \\\n",
    "MC Dropout (Monte Carlo Dropout) is an extension of the dropout technique that introduces a form of approximate Bayesian inference during inference time. Instead of only using the model once for prediction, MC Dropout performs multiple forward passes with dropout enabled and computes predictions based on the average or ensemble of the predictions across the passes. This enables the model to capture the uncertainty associated with its predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d275266",
   "metadata": {},
   "source": [
    "#### 8. Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc72af7",
   "metadata": {},
   "source": [
    "- **a) Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a329e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 15s 23ms/step - loss: 2.0709 - accuracy: 0.2355 - val_loss: 1.8794 - val_accuracy: 0.3044\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.8623 - accuracy: 0.3163 - val_loss: 1.8061 - val_accuracy: 0.3380\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.7978 - accuracy: 0.3447 - val_loss: 1.7615 - val_accuracy: 0.3452\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.7321 - accuracy: 0.3688 - val_loss: 1.6836 - val_accuracy: 0.3917\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.6809 - accuracy: 0.3909 - val_loss: 1.6462 - val_accuracy: 0.3989\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.6438 - accuracy: 0.4082 - val_loss: 1.6311 - val_accuracy: 0.4113\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.6230 - accuracy: 0.4178 - val_loss: 1.5983 - val_accuracy: 0.4350\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1.5898 - accuracy: 0.4301 - val_loss: 1.5707 - val_accuracy: 0.4384\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.5570 - accuracy: 0.4411 - val_loss: 1.5949 - val_accuracy: 0.4286\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.5467 - accuracy: 0.4483 - val_loss: 1.5572 - val_accuracy: 0.4436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1983580ea00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "# Load cifar-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) =keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel value between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "model.add(keras.layers.Flatten(input_shape = (32, 32, 3)))\n",
    "\n",
    "# Add 20  hidden layer with 100 neurons each, using He initialization and ELU activation\n",
    "for i in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation='elu', kernel_initializer = 'he_normal'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size= 128, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7ddb4",
   "metadata": {},
   "source": [
    "- **b) Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db23ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 8s 14ms/step - loss: 1.8567 - accuracy: 0.3392 - val_loss: 1.6712 - val_accuracy: 0.4127\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.6169 - accuracy: 0.4263 - val_loss: 1.5641 - val_accuracy: 0.4503\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.5304 - accuracy: 0.4573 - val_loss: 1.5309 - val_accuracy: 0.4563\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.4717 - accuracy: 0.4773 - val_loss: 1.4903 - val_accuracy: 0.4720\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.4294 - accuracy: 0.4909 - val_loss: 1.4601 - val_accuracy: 0.4812\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.3898 - accuracy: 0.5051 - val_loss: 1.4293 - val_accuracy: 0.4943\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.3587 - accuracy: 0.5147 - val_loss: 1.4211 - val_accuracy: 0.4960\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.3323 - accuracy: 0.5263 - val_loss: 1.4275 - val_accuracy: 0.4953\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.3035 - accuracy: 0.5362 - val_loss: 1.3998 - val_accuracy: 0.5054\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.2757 - accuracy: 0.5451 - val_loss: 1.4171 - val_accuracy: 0.4983\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.2531 - accuracy: 0.5547 - val_loss: 1.4096 - val_accuracy: 0.4990\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.2314 - accuracy: 0.5611 - val_loss: 1.4123 - val_accuracy: 0.5008\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.2100 - accuracy: 0.5677 - val_loss: 1.3668 - val_accuracy: 0.5177\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.1860 - accuracy: 0.5765 - val_loss: 1.3747 - val_accuracy: 0.5138\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.1643 - accuracy: 0.5841 - val_loss: 1.3816 - val_accuracy: 0.5174\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.1476 - accuracy: 0.5889 - val_loss: 1.4301 - val_accuracy: 0.5062\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.1309 - accuracy: 0.5948 - val_loss: 1.3899 - val_accuracy: 0.5094\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.1109 - accuracy: 0.6028 - val_loss: 1.3764 - val_accuracy: 0.5200\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Define the deep neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer and sparse categorical cross-entropy loss\n",
    "model.compile(optimizer=keras.optimizers.Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a614a2b3",
   "metadata": {},
   "source": [
    "- **c) Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4c4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 14s 20ms/step - loss: 1.6472 - accuracy: 0.4161 - val_loss: 1.4942 - val_accuracy: 0.4689\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.4327 - accuracy: 0.4906 - val_loss: 1.4250 - val_accuracy: 0.4942\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 1.3392 - accuracy: 0.5265 - val_loss: 1.3825 - val_accuracy: 0.5132\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 1.2764 - accuracy: 0.5475 - val_loss: 1.3649 - val_accuracy: 0.5141\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 1.2254 - accuracy: 0.5653 - val_loss: 1.3620 - val_accuracy: 0.5172\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.1778 - accuracy: 0.5803 - val_loss: 1.3548 - val_accuracy: 0.5188\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.1378 - accuracy: 0.5963 - val_loss: 1.3390 - val_accuracy: 0.5269\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.1056 - accuracy: 0.6064 - val_loss: 1.3405 - val_accuracy: 0.5292\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.0735 - accuracy: 0.6190 - val_loss: 1.3521 - val_accuracy: 0.5331\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.0411 - accuracy: 0.6295 - val_loss: 1.3519 - val_accuracy: 0.5284\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1.0102 - accuracy: 0.6443 - val_loss: 1.3781 - val_accuracy: 0.5289\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.9833 - accuracy: 0.6491 - val_loss: 1.3847 - val_accuracy: 0.5284\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load cifar-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1 \n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Define the deep neural network model with Batch Normalization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32,32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu',kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "# Compile the model with Nadam optimizer and sparse categorical cross entropy loss\n",
    "model.compile(optimizer=keras.optimizers.Nadam(),loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2615b8",
   "metadata": {},
   "source": [
    "- **d) Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4926f8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "391/391 [==============================] - 10s 19ms/step - loss: 2.1922 - accuracy: 0.2093 - val_loss: 1.8620 - val_accuracy: 0.3294\n",
      "Epoch 2/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9214 - accuracy: 0.2927 - val_loss: 1.7997 - val_accuracy: 0.3744\n",
      "Epoch 3/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.8443 - accuracy: 0.3232 - val_loss: 1.7905 - val_accuracy: 0.3989\n",
      "Epoch 4/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7883 - accuracy: 0.3505 - val_loss: 1.7515 - val_accuracy: 0.4083\n",
      "Epoch 5/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7423 - accuracy: 0.3667 - val_loss: 1.7885 - val_accuracy: 0.4319\n",
      "Epoch 6/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7092 - accuracy: 0.3819 - val_loss: 1.7553 - val_accuracy: 0.4397\n",
      "Epoch 7/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6799 - accuracy: 0.3920 - val_loss: 1.7677 - val_accuracy: 0.4384\n",
      "Epoch 8/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6629 - accuracy: 0.3992 - val_loss: 1.7014 - val_accuracy: 0.4500\n",
      "Epoch 9/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6404 - accuracy: 0.4070 - val_loss: 1.7180 - val_accuracy: 0.4508\n",
      "Epoch 10/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6239 - accuracy: 0.4163 - val_loss: 1.7180 - val_accuracy: 0.4617\n",
      "Epoch 11/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.6090 - accuracy: 0.4202 - val_loss: 1.7315 - val_accuracy: 0.4686\n",
      "Epoch 12/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5957 - accuracy: 0.4252 - val_loss: 1.6788 - val_accuracy: 0.4667\n",
      "Epoch 13/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5886 - accuracy: 0.4283 - val_loss: 1.7405 - val_accuracy: 0.4684\n",
      "Epoch 14/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5690 - accuracy: 0.4349 - val_loss: 1.6708 - val_accuracy: 0.4655\n",
      "Epoch 15/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5656 - accuracy: 0.4365 - val_loss: 1.6268 - val_accuracy: 0.4844\n",
      "Epoch 16/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5511 - accuracy: 0.4416 - val_loss: 1.6803 - val_accuracy: 0.4745\n",
      "Epoch 17/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5409 - accuracy: 0.4483 - val_loss: 1.6874 - val_accuracy: 0.4771\n",
      "Epoch 18/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5336 - accuracy: 0.4504 - val_loss: 1.6465 - val_accuracy: 0.4857\n",
      "Epoch 19/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5239 - accuracy: 0.4539 - val_loss: 1.6730 - val_accuracy: 0.4760\n",
      "Epoch 20/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5180 - accuracy: 0.4538 - val_loss: 1.6961 - val_accuracy: 0.4819\n",
      "Epoch 21/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5124 - accuracy: 0.4568 - val_loss: 1.7493 - val_accuracy: 0.4903\n",
      "Epoch 22/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5069 - accuracy: 0.4589 - val_loss: 1.6978 - val_accuracy: 0.4858\n",
      "Epoch 23/25\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.4983 - accuracy: 0.4629 - val_loss: 1.6428 - val_accuracy: 0.4916\n",
      "Epoch 24/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4973 - accuracy: 0.4625 - val_loss: 1.6641 - val_accuracy: 0.4894\n",
      "Epoch 25/25\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4830 - accuracy: 0.4690 - val_loss: 1.6454 - val_accuracy: 0.4919\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.6454 - accuracy: 0.4919\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 1.6454 - accuracy: 0.4919\n",
      "Accuracy with Alpha Dropout: 0.4918999969959259\n",
      "Accuracy with MC Dropout: 0.4918999969959259\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Define the deep neural network model with Alpha Dropout\n",
    "model_alpha_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.AlphaDropout(0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.AlphaDropout(0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.AlphaDropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer and sparse categorical cross-entropy loss\n",
    "model_alpha_dropout.compile(optimizer=keras.optimizers.Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with alpha dropout\n",
    "history_alpha_dropout = model_alpha_dropout.fit(x_train, y_train, epochs=25, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model with alpha dropout on the test data\n",
    "_, accuracy_alpha_dropout = model_alpha_dropout.evaluate(x_test, y_test)\n",
    "\n",
    "# Create a copy of the model for MC Dropout\n",
    "model_mc_dropout = keras.models.clone_model(model_alpha_dropout)\n",
    "model_mc_dropout.set_weights(model_alpha_dropout.get_weights())\n",
    "\n",
    "# Compile the model with MC Dropout and sparse categorical cross-entropy loss\n",
    "model_mc_dropout.compile(optimizer=keras.optimizers.Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model with MC Dropout on the test data\n",
    "_, accuracy_mc_dropout = model_mc_dropout.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy with Alpha Dropout:\", accuracy_alpha_dropout)\n",
    "print(\"Accuracy with MC Dropout:\", accuracy_mc_dropout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
