{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a365d0",
   "metadata": {},
   "source": [
    "## Assignment Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6b7b7",
   "metadata": {},
   "source": [
    "#### 1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0c1a7",
   "metadata": {},
   "source": [
    "**Ans:** Prior probability refers to the initial or initial belief or probability assigned to an event or hypothesis before any evidence or data is taken into account. It represents the probability of an event based on existing knowledge or information before considering any new observations or evidence.\n",
    "\n",
    "Here's an example to illustrate prior probability: \\\n",
    "Suppose you have a bag that contains 100 balls, of which 60 are red and 40 are blue. Without looking into the bag, you randomly select a ball. The prior probability of selecting a red ball would be 60/100 or 0.6 (60% chance) because, based on the information you have, there is a higher likelihood of selecting a red ball."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028bb45",
   "metadata": {},
   "source": [
    "#### 2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49430b3c",
   "metadata": {},
   "source": [
    "**Ans:** Posterior probability, also known as the updated probability, refers to the probability of an event or hypothesis after considering new evidence or data. It is derived by combining the prior probability with the observed evidence using Bayes' theorem.\n",
    "\n",
    "Here's an example to illustrate posterior probability:\n",
    "\n",
    "Let's say you have a medical test to detect a certain disease. The test has a known accuracy rate: if a person has the disease, the test correctly identifies them as positive 95% of the time (true positive rate), and if a person does not have the disease, the test correctly identifies them as negative 90% of the time (true negative rate).\n",
    "\n",
    "Now, suppose the prevalence of the disease in the general population is 1% (prior probability). You take the test and it comes back positive. Given this new evidence, you want to calculate the posterior probability that you actually have the disease.\n",
    "\n",
    "Using Bayes' theorem, you can combine the prior probability with the test result to calculate the posterior probability:\n",
    "\n",
    "Posterior probability = (Prior probability * True positive rate) / [(Prior probability * True positive rate) + [(1 - Prior probability) * False positive rate]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a414a",
   "metadata": {},
   "source": [
    "#### 3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14065f70",
   "metadata": {},
   "source": [
    "**Ans:** Likelihood probability, represents the probability of obtaining the observed data. likelihood probability focuses on the relationship between the data and the parameters of a statistical model.\n",
    "\n",
    "Here's an example to illustrate likelihood probability:\n",
    "\n",
    "Suppose you have a coin, and you want to determine if it is a fair coin (50% chance of landing on heads and 50% chance of landing on tails) or biased towards heads. You decide to conduct an experiment and flip the coin 10 times, observing that it lands on heads 8 times and tails 2 times.\n",
    "\n",
    "To calculate the likelihood probability, you consider the probability of obtaining these specific outcomes (8 heads and 2 tails) given different hypotheses about the fairness of the coin.\n",
    "\n",
    "Let's consider two hypotheses:\n",
    "H1: The coin is fair (50% chance of heads and 50% chance of tails).\n",
    "H2: The coin is biased towards heads (60% chance of heads and 40% chance of tails).\n",
    "\n",
    "For hypothesis H1 (fair coin), the likelihood probability can be calculated as:\n",
    "Likelihood (H1) = Probability of getting 8 heads and 2 tails with a fair coin = (0.5^8) * (0.5^2) = 0.00390625.\n",
    "\n",
    "For hypothesis H2 (biased coin), the likelihood probability can be calculated as:\n",
    "Likelihood (H2) = Probability of getting 8 heads and 2 tails with a biased coin = (0.6^8) * (0.4^2) = 0.006047.\n",
    "\n",
    "In this example, the likelihood probability indicates how well each hypothesis (fair coin or biased coin) explains the observed data of 8 heads and 2 tails. The higher the likelihood probability, the more likely the hypothesis is to explain the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201c665",
   "metadata": {},
   "source": [
    "#### 4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3e20c",
   "metadata": {},
   "source": [
    "**Ans:** The Naïve Bayes classifier is a simple and commonly used probabilistic classification algorithm that applies Bayes' theorem with a strong assumption of feature independence. It is named \"naïve\" because it assumes that the features are conditionally independent of each other given the class label, which is often an oversimplified assumption in practice but can still yield surprisingly good results in many real-world applications.\n",
    "\n",
    "The Naïve Bayes classifier is primarily used for text classification tasks, such as spam detection, sentiment analysis, document categorization, and topic classification\n",
    "\n",
    "**The algorithm works as follows:**\n",
    "\n",
    "- Training phase: Given a labeled training dataset, the Naïve Bayes classifier calculates the prior probability of each class label and the likelihood probability of each feature value given each class label. It builds a probability model based on these probabilities.\n",
    "\n",
    "- Classification phase: When classifying new, unseen data, the Naïve Bayes classifier calculates the posterior probability of each class label given the observed feature values. It assigns the class label with the highest posterior probability to the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970d490",
   "metadata": {},
   "source": [
    "#### 5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc717d1",
   "metadata": {},
   "source": [
    "**Ans:** The Optimal Bayes classifier, is a theoretical concept in machine learning and statistics. It represents the best possible classifier that can be achieved under the assumption of known probability distributions and zero misclassification costs.\n",
    "\n",
    "The Optimal Bayes classifier is derived directly from Bayes' theorem and makes decisions based on the posterior probabilities of class labels given the observed features. It assigns the input data to the class label with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281141b",
   "metadata": {},
   "source": [
    "#### 6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35450673",
   "metadata": {},
   "source": [
    "**Ans:** Two features of Bayesian learning methods are:\n",
    "\n",
    "**Probabilistic Modeling:** Bayesian learning methods make use of probabilistic models to represent uncertainty and variability in data. Rather than providing a single \"best\" solution, Bayesian methods provide a probability distribution over possible solutions. This allows for a more comprehensive and flexible representation of knowledge and enables reasoning with uncertainty.\n",
    "\n",
    "**Prior Knowledge Integration:** Bayesian learning methods incorporate prior knowledge or beliefs into the learning process. This prior knowledge, in the form of prior probabilities or prior distributions, represents existing information or assumptions about the problem domain. By combining prior knowledge with observed data, Bayesian methods can update and refine the beliefs or probabilities associated with different hypotheses or parameters. This allows for a principled way to incorporate existing knowledge into the learning process and can be particularly useful when dealing with limited or noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a2ca4",
   "metadata": {},
   "source": [
    "#### 7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68f97e",
   "metadata": {},
   "source": [
    "**Ans:** Consistent learners, are machine learning algorithms or models that converge to the true underlying function or data-generating distribution as the amount of training data increases indefinitely. In other words, a consistent learner will produce increasingly accurate predictions or estimates as more data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa5c25",
   "metadata": {},
   "source": [
    "#### 8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe2130",
   "metadata": {},
   "source": [
    "**Ans:** Two strengths of the Bayes classifier are:\n",
    "\n",
    "**Probabilistic Framework:** The Bayes classifier operates within a probabilistic framework, allowing it to provide a measure of uncertainty for its predictions. Instead of providing a deterministic classification, it assigns probabilities to different class labels, reflecting the confidence or likelihood of each label. This probabilistic output is valuable in various applications, such as risk assessment, decision-making, and understanding the uncertainty associated with predictions.\n",
    "\n",
    "**Efficient with High-Dimensional Data:** The Bayes classifier, particularly the Naïve Bayes variant, is known for its efficiency in handling high-dimensional data. It assumes independence between features, simplifying the computational complexity by considering each feature separately. This feature independence assumption enables the classifier to handle a large number of features effectively, even when the data dimensionality is much higher than the number of available training instances. Consequently, the Bayes classifier can be well-suited for text classification and other applications involving high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26babd",
   "metadata": {},
   "source": [
    "#### 9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b83234",
   "metadata": {},
   "source": [
    "**Ans:** Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "**Strong Independence Assumption:** The Naïve Bayes classifier, in particular, assumes that all features are conditionally independent of each other given the class label. While this assumption simplifies the model and enables computational efficiency, it may not hold in many real-world scenarios. In practice, there are often correlations and dependencies between features, and assuming independence can lead to suboptimal performance. \n",
    "\n",
    "**Limited Model Expressiveness:** The Bayes classifier's modeling capabilities are limited by the assumptions and structure of the chosen probability distribution. The classifier's performance heavily relies on the correct specification of the underlying probability distributions and the assumptions made about the data. If the true data distribution deviates significantly from the assumed distribution or if the assumptions are violated, the Bayes classifier may fail to capture the true underlying patterns effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9679ae7",
   "metadata": {},
   "source": [
    "#### 10. Explain how Naïve Bayes classifier is used for\n",
    " - **1.Text classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b75e6",
   "metadata": {},
   "source": [
    "**Ans:** **Text Classification:**\n",
    "Text classification involves categorizing or assigning predefined labels or categories to a given piece of text based on its content. The Naïve Bayes classifier is well-suited for this task. In text classification, the classifier is trained on a labeled dataset where each text document is associated with a specific category. The classifier learns the statistical relationships between the words or features in the documents and their corresponding categories.\n",
    "\n",
    "During the training phase, the Naïve Bayes classifier estimates the prior probabilities of each category and the likelihood probabilities of the words or features occurring in each category. These probabilities are used to build a probability model. In the classification phase, the classifier applies Bayes' theorem to calculate the posterior probabilities of each category given the observed words or features in a new document. The document is then assigned the category with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9bdac",
   "metadata": {},
   "source": [
    "- **2.Spam Filtering:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cc196",
   "metadata": {},
   "source": [
    "**Ans:** **Spam Filtering:**\n",
    "Spam filtering involves distinguishing between legitimate emails and unsolicited or unwanted spam emails. The Naïve Bayes classifier is particularly effective for this task due to its ability to handle high-dimensional feature spaces and the abundance of labeled training data available for spam detection.\n",
    "In spam filtering, the Naïve Bayes classifier is trained on a dataset of emails labeled as spam or non-spam (ham). The classifier learns the statistical patterns or features associated with each category. These features can include specific words, phrases, or other characteristics found in the email content, metadata, or header information.\n",
    "\n",
    "During the training phase, the classifier estimates the prior probabilities of spam and non-spam emails and the likelihood probabilities of the features occurring in each category. In the classification phase, the classifier applies Bayes' theorem to calculate the posterior probabilities of an incoming email being spam or non-spam based on the observed features. The email is then classified as spam or non-spam based on the category with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a1251",
   "metadata": {},
   "source": [
    "- **3.Market Sentiment Analysis:**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9914e1",
   "metadata": {},
   "source": [
    "**Ans:** **Market Sentiment Analysis:**\n",
    "Market sentiment analysis aims to assess and classify the sentiment or opinion expressed in textual data, such as social media posts, customer reviews, or news articles, regarding a particular financial instrument, company, or market. The Naïve Bayes classifier can be applied to perform sentiment analysis by categorizing the text into positive, negative, or neutral sentiment categories. \\\n",
    "To use the Naïve Bayes classifier for sentiment analysis, a labeled dataset is required, where each text is annotated with its corresponding sentiment label. The classifier learns the statistical patterns or features associated with positive, negative, or neutral sentiments.\n",
    "\n",
    "During the training phase, the classifier estimates the prior probabilities of each sentiment category and the likelihood probabilities of the features occurring in each sentiment category. In the classification phase, the classifier applies Bayes' theorem to calculate the posterior probabilities of each sentiment category given the observed features in a new text. The text is then assigned the sentiment category with the highest posterior probability, indicating the sentiment expressed in the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
